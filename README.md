# Scala Spark Distributed Data Pipeline

[![Scala CI](https://github.com/galafis/scala-spark-distributed-data-pipeline/actions/workflows/ci.yml/badge.svg)](https://github.com/galafis/scala-spark-distributed-data-pipeline/actions/workflows/ci.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
![Scala](https://img.shields.io/badge/Scala-DC322F?style=for-the-badge&logo=scala&logoColor=white) ![Apache Spark](https://img.shields.io/badge/Apache%20Spark-E25A1C?style=for-the-badge&logo=apache-spark&logoColor=white) ![Big Data](https://img.shields.io/badge/Big_Data-FF6F00?style=for-the-badge)

---

## üáßüá∑ Pipeline de Dados Distribu√≠do com Scala e Spark

Pipeline ETL de **n√≠vel empresarial** constru√≠do com **Scala** e **Apache Spark** para processamento distribu√≠do de grandes volumes de dados. Demonstra padr√µes modernos de engenharia de dados, incluindo processamento em batch, otimiza√ß√µes de performance e integra√ß√£o com data lakes.

### üéØ Objetivo

Fornecer uma arquitetura completa e escal√°vel para pipelines de dados que processam **terabytes de dados** com alta performance, demonstrando as melhores pr√°ticas de Spark em produ√ß√£o.

### üåü Por que Scala + Spark?

| Caracter√≠stica | Scala/Spark | Python/Spark | Java/Spark |
|----------------|-------------|--------------|------------|
| **Performance** | Excelente (nativo) | Boa (overhead) | Excelente |
| **Type Safety** | ‚úÖ Forte | ‚ùå Fraca | ‚úÖ Forte |
| **Expressividade** | ‚úÖ Alta | ‚úÖ Alta | ‚ùå M√©dia |
| **Ecosystem** | Spark nativo | Pandas, NumPy | Enterprise |
| **Learning Curve** | M√©dio-Alto | Baixo | M√©dio |

### üìä Casos de Uso

1. **E-commerce**: Processar 100M+ eventos de clickstream diariamente
2. **Fintech**: Agrega√ß√£o de transa√ß√µes para detec√ß√£o de fraude
3. **Telecom**: An√°lise de CDRs (Call Detail Records) em escala
4. **IoT**: Processamento de telemetria de milh√µes de dispositivos
5. **Marketing**: ETL de dados de m√∫ltiplas fontes para data warehouse

### üèóÔ∏è Arquitetura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Data Sources‚îÇ (S3, HDFS, Kafka, JDBC)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Spark Cluster (YARN/K8s)       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ Driver ‚îÇ‚Üí ‚îÇExecutor‚îÇ‚Üí ‚îÇ... ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                  ‚îÇ
‚îÇ  - Extract (read)                ‚îÇ
‚îÇ  - Transform (map/filter/agg)    ‚îÇ
‚îÇ  - Load (write)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ Data Lake/DW  ‚îÇ (Parquet, Delta, Iceberg)
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üíª C√≥digo Principal

```scala
package pipeline

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

object DataPipeline {
  
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("Distributed Data Pipeline")
      .config("spark.sql.adaptive.enabled", "true")
      .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
      .getOrCreate()
    
    import spark.implicits._
    
    // Extract
    val rawData = extractData(spark, "s3a://bucket/raw-data/")
    
    // Transform
    val transformedData = transformData(rawData)
    
    // Load
    loadData(transformedData, "s3a://bucket/processed-data/")
    
    spark.stop()
  }
  
  def extractData(spark: SparkSession, path: String): DataFrame = {
    spark.read
      .option("header", "true")
      .option("inferSchema", "true")
      .parquet(path)
  }
  
  def transformData(df: DataFrame): DataFrame = {
    df.filter($"amount" > 0)
      .withColumn("year", year($"date"))
      .withColumn("month", month($"date"))
      .groupBy("year", "month", "category")
      .agg(
        sum("amount").as("total_amount"),
        count("*").as("transaction_count"),
        avg("amount").as("avg_amount")
      )
      .orderBy("year", "month")
  }
  
  def loadData(df: DataFrame, path: String): Unit = {
    df.write
      .mode("overwrite")
      .partitionBy("year", "month")
      .parquet(path)
  }
}
```

### üöÄ Instala√ß√£o e Execu√ß√£o

#### Pr√©-requisitos

- **Java JDK 11** (recomendado para testes)
  - ‚ö†Ô∏è Nota: Java 17 √© suportado mas h√° incompatibilidades conhecidas com Spark 3.5.0 em testes de I/O
- SBT 1.9.x
- Apache Spark 3.5.0 (opcional, para cluster)

#### Instala√ß√£o

```bash
# Clone o reposit√≥rio
git clone https://github.com/galafis/scala-spark-distributed-data-pipeline.git
cd scala-spark-distributed-data-pipeline

# Build com sbt
sbt clean compile package

# Criar JAR com todas as depend√™ncias (assembly)
sbt assembly
```

#### Execu√ß√£o

```bash
# Executar localmente
spark-submit \
  --class pipeline.DataPipeline \
  --master local[*] \
  target/scala-2.12/scala-spark-distributed-data-pipeline_2.12-1.0.0.jar \
  input_data.csv \
  output_results/

# Executar em cluster YARN
spark-submit \
  --class pipeline.DataPipeline \
  --master yarn \
  --deploy-mode cluster \
  --num-executors 10 \
  --executor-memory 4G \
  --executor-cores 2 \
  target/scala-2.12/scala-spark-distributed-data-pipeline_2.12-1.0.0.jar \
  s3a://bucket/input/ \
  s3a://bucket/output/
```

### üß™ Testes

O projeto inclui uma su√≠te completa de testes unit√°rios e de integra√ß√£o.

#### Executar Testes

```bash
# Executar todos os testes
sbt test

# Executar teste espec√≠fico
sbt "testOnly pipeline.DataPipelineTest"

# Executar testes com cobertura
sbt coverage test coverageReport

# Ver relat√≥rio de cobertura
open target/scala-2.12/scoverage-report/index.html
```

#### Estrutura de Testes

- **SparkTest**: Classe base que configura SparkSession para testes
- **DataPipelineTest**: 17+ testes cobrindo todas as fun√ß√µes do pipeline
  - Testes de extra√ß√£o (CSV, Parquet, JSON)
  - Testes de transforma√ß√£o (limpeza, agrega√ß√£o, filtros)
  - Testes de carga (diferentes formatos e modos)
  - Teste de integra√ß√£o end-to-end

#### Formata√ß√£o de C√≥digo

```bash
# Verificar formata√ß√£o
sbt scalafmtCheckAll

# Formatar c√≥digo automaticamente
sbt scalafmtAll
```

### üìà Otimiza√ß√µes de Performance

```scala
// 1. Broadcast Join para tabelas pequenas
val largeDf = spark.read.parquet("large_table")
val smallDf = spark.read.parquet("small_table")
val result = largeDf.join(broadcast(smallDf), "key")

// 2. Reparticionamento estrat√©gico
val optimized = df.repartition(200, $"partition_key")

// 3. Cache de DataFrames intermedi√°rios
val cachedDf = df.filter($"active" === true).cache()

// 4. Predicate Pushdown
val filtered = spark.read
  .parquet("data")
  .filter($"date" >= "2025-01-01")  // Filtro antes de ler

// 5. Column Pruning
val selected = df.select("id", "name", "amount")  // Apenas colunas necess√°rias
```

### üéì Conceitos Avan√ßados

#### Transforma√ß√µes Lazy vs Actions

```scala
// Lazy (n√£o executa)
val transformed = df.filter($"amount" > 100).select("id", "amount")

// Action (executa todo o pipeline)
transformed.count()  // Trigger
transformed.show()   // Trigger
transformed.write.parquet("output")  // Trigger
```

#### Particionamento Eficiente

```scala
// Particionar por data para queries temporais
df.write
  .partitionBy("year", "month", "day")
  .parquet("output")

// Query otimizada
spark.read.parquet("output")
  .filter($"year" === 2025 && $"month" === 10)  // L√™ apenas parti√ß√µes relevantes
```

### üß™ Testes (Tests)

Este projeto possui uma su√≠te completa de testes automatizados. Veja a se√ß√£o de testes acima para mais detalhes.

Exemplo de teste:

```scala
test("transformData should remove duplicates") {
  val testData = Seq(
    ("Order1", Date.valueOf("2025-01-15"), "Electronics", "Customer1", 100.0, 50.0, 2),
    ("Order1", Date.valueOf("2025-01-15"), "Electronics", "Customer1", 100.0, 50.0, 2)
  ).toDF("Order ID", "Order Date", "Category", "Customer ID", "Sales", "Profit", "Quantity")

  val result = DataPipeline.transformData(testData)
  
  result.count() should be(1)
}
```

### üìä Monitoramento

```scala
// M√©tricas customizadas
val metrics = df.agg(
  count("*").as("total_records"),
  sum("amount").as("total_amount"),
  avg("amount").as("avg_amount"),
  min("date").as("min_date"),
  max("date").as("max_date")
).collect()(0)

println(s"Processed ${metrics.getLong(0)} records")
println(s"Total amount: ${metrics.getDouble(1)}")
```

### üîß Troubleshooting (Solu√ß√£o de Problemas)

#### Erro: "OutOfMemoryError"

```bash
# Aumentar mem√≥ria do executor
spark-submit \
  --executor-memory 8G \
  --driver-memory 4G \
  --conf spark.memory.fraction=0.8 \
  ...
```

#### Erro: Testes falhando com Java 17

‚ö†Ô∏è **Problema Conhecido**: Incompatibilidade entre Spark 3.5.0 e Java 17

```bash
# Solu√ß√£o 1: Usar Java 11 (Recomendado)
export JAVA_HOME=/path/to/java11
sbt test

# Solu√ß√£o 2: Aguardar Spark 3.6.0+ com suporte completo a Java 17

# Solu√ß√£o 3: Executar em produ√ß√£o (cluster j√° configurado)
# Os testes locais podem falhar, mas o c√≥digo funciona em cluster
```

**Contexto**: Java 17 introduziu mudan√ßas no sistema de m√≥dulos que afetam o acesso interno do Spark a classes sun.*. Embora o c√≥digo compile e execute em clusters de produ√ß√£o, alguns testes locais podem falhar.

#### Erro: "Unable to load native-hadoop library"

```bash
# Adicionar depend√™ncias Hadoop no classpath
export HADOOP_HOME=/path/to/hadoop
export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH
```

#### Performance lenta em agrega√ß√µes

```scala
// Aumentar parti√ß√µes antes de agregar
df.repartition(200, $"partition_key")
  .groupBy("key")
  .agg(...)

// Ou usar coalesce ap√≥s agregar
result.coalesce(10).write.parquet("output")
```

#### Problema com tipos de dados

```scala
// Definir schema explicitamente
val schema = StructType(Seq(
  StructField("id", LongType, nullable = false),
  StructField("name", StringType, nullable = true),
  StructField("amount", DoubleType, nullable = false)
))

spark.read.schema(schema).csv("data.csv")
```

#### Testes falhando

```bash
# Limpar cache e executar novamente
sbt clean test

# Executar com mais mem√≥ria
export SBT_OPTS="-Xmx4G -XX:MaxMetaspaceSize=512M"
sbt test
```

### üîó Recursos

- [Spark Scala API](https://spark.apache.org/docs/latest/api/scala/)
- [Scala Documentation](https://docs.scala-lang.org/)
- [High Performance Spark (Book)](https://www.oreilly.com/library/view/high-performance-spark/9781491943199/)

### ü§ù Como Contribuir

Contribui√ß√µes s√£o bem-vindas! Por favor, leia nosso [Guia de Contribui√ß√£o](CONTRIBUTING.md) antes de enviar pull requests.

1. Fa√ßa um fork do reposit√≥rio
2. Crie sua feature branch (`git checkout -b feature/NovaFuncionalidade`)
3. Commit suas mudan√ßas (`git commit -m 'feat: Adiciona nova funcionalidade'`)
4. Push para a branch (`git push origin feature/NovaFuncionalidade`)
5. Abra um Pull Request

### üìÑ Licen√ßa

Este projeto est√° licenciado sob a Licen√ßa MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

---

## üá¨üáß Scala Spark Distributed Data Pipeline

Enterprise-grade ETL pipeline built with **Scala** and **Apache Spark** for distributed processing of large data volumes.

### üöÄ Quick Start

```bash
# Clone and build
git clone https://github.com/galafis/scala-spark-distributed-data-pipeline.git
cd scala-spark-distributed-data-pipeline
sbt clean compile test

# Run with sample data
sbt "run input_data.csv output_results/"

# Create assembly JAR
sbt assembly

# Run with Spark
spark-submit \
  --class pipeline.DataPipeline \
  --master local[*] \
  target/scala-2.12/scala-spark-distributed-data-pipeline_2.12-1.0.0.jar \
  input_data.csv \
  output_results/
```

### üß™ Testing

```bash
# Run all tests
sbt test

# Run with coverage
sbt coverage test coverageReport
```

### ü§ù Contributing

Contributions are welcome! Please read our [Contributing Guidelines](CONTRIBUTING.md) before submitting pull requests.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'feat: Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

**Author:** Gabriel Demetrios Lafis  
**Repository:** [github.com/galafis/scala-spark-distributed-data-pipeline](https://github.com/galafis/scala-spark-distributed-data-pipeline)  
**Status:** [![Scala CI](https://github.com/galafis/scala-spark-distributed-data-pipeline/actions/workflows/ci.yml/badge.svg)](https://github.com/galafis/scala-spark-distributed-data-pipeline/actions/workflows/ci.yml)
